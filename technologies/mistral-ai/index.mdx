---
title: "Mistral AI"
description: "Mistral AI is a Paris-based AI company building a portfolio of high-performance large language and multimodal models. Their work includes open-weight models, reasoning systems, code-focused models, and enterprise services with strong emphasis on transparency, fine-tuning, and European AI sovereignty."
---

# Mistral AI

Mistral AI develops a wide spectrum of AI models and services, enabling developers, researchers, and businesses to build, deploy, and fine-tune large language and multimodal models.  
The company focuses on **open weights**, **reasoning capability**, **multimodality**, and **enterprise-grade features** such as long context windows, domain-specific deployments, and fine-tuning options.

| General     |                                                        |
| ----------- | ------------------------------------------------------ |
| Founded     | 2023 (Paris, France)                                   |
| Founders    | Arthur Mensch, Guillaume Lample, Timothée Lacroix      |
| Valuation   | ~€14 billion (Series C, September 2025)              |
| Investors   | ASML (largest shareholder), Microsoft, CMA CGM, others |
| Type        | Large language and multimodal models                   |

---

### Mistral Models

Mistral divides its lineup into **open models** (weights freely available) and **premier models** (API-first, enterprise-grade).  
Here are the most important families:

- **Mistral 7B** – Compact, open-weight dense model for efficient deployment.  
- **Mixtral 8×7B / 8×22B** – Sparse mixture-of-experts models balancing performance and cost.  
- **Mistral NeMo 12B** – Strong open-weight model for multilingual and reasoning tasks.  
- **Codestral** – Code-oriented models for software engineering and developer tools.  
- **Pixtral** – Multimodal family supporting text + image inputs (e.g. Pixtral-12B, Pixtral Large).  
- **Magistral** – Reasoning-focused models; *Magistral Small* (open-weight) and *Magistral Medium* (enterprise).  
- **Mistral Medium 3 / 3.1** – Premier multimodal models with ~131K context length, enterprise-grade APIs.  
- **Mistral Large / Large 2 (123B)** – Very large dense models with long context, available via API.  
- **Specialized Models** – OCR models (e.g. mistral-ocr-2503), embeddings, moderation, and speech (Voxtral).  

---

### La Plateforme

Mistral provides its own developer and enterprise platform, called **La Plateforme**, where you can:

- Access **Premier Models** via API  
- Manage **fine-tuning** (LoRA adapters, serverless training)  
- Handle **billing, versioning, and deployments**  
- Explore **model catalogs** and track deprecations  

- [La Plateforme Overview](https://docs.mistral.ai/getting-started/models/models_overview/)  
- [Mistral Docs](https://docs.mistral.ai/)  
- [Model Weights](https://docs.mistral.ai/getting-started/models/weights/)  

---

### Mistral AI - Boilerplates

Get started quickly with open-weight or API integrations:

- [Mistral 7B Inference Example](https://huggingface.co/mistralai/Mistral-7B-v0.1) Run inference on the base 7B model  
- [Mixtral Example](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) Sparse MoE demo  
- [Codestral Demo](https://huggingface.co/mistralai/Codestral-22B-v0.1) Code generation examples   

---

### Mistral AI - Tutorials

Learn how to build with Mistral’s models:

- [Using La Plateforme API](https://docs.mistral.ai/api) Guide for enterprise API usage  
- [Deploying Mistral on Azure AI Foundry](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/deepening-our-partnership-with-mistral-ai-on-azure-ai-foundry/4434656) Integration with Microsoft services  
- [Working with Pixtral](https://mistral.ai/news/pixtral-12b) Multimodal workflows with text + images  

---

### Mistral AI

Most important links to explore Mistral’s ecosystem:

- [Mistral AI Website](https://mistral.ai/)  
- [Mistral News](https://mistral.ai/news/)   
- [Mistral GitHub](https://github.com/mistralai)  
- [Mistral Docs](https://docs.mistral.ai/)  

---
